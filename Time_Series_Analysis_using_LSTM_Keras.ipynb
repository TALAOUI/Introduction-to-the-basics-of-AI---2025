{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TALAOUI/Introduction-to-the-basics-of-AI---2025/blob/main/Time_Series_Analysis_using_LSTM_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "dgawlik_nyse_path = kagglehub.dataset_download('dgawlik/nyse')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "06FskRF6U0b8",
        "outputId": "bd30fb5a-fa14-48cd-a3d2-a1639e7bc8ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.7), please consider upgrading to the latest version (0.3.8).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/dgawlik/nyse?dataset_version_number=3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30.7M/30.7M [00:00<00:00, 70.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "metadata": {
        "_uuid": "3d7c6a3bd655da50b56413fc3e2e10c2531ef842",
        "id": "R4Ap73FXU0b9"
      },
      "cell_type": "markdown",
      "source": [
        "The type of Neural Network designed to handle sequence dependence is called recurrent neural networks(RNN).\n",
        "\n",
        "The Long Short-Term Memory network or LSTM network is a type of recurrent neural network used in deep learning because very large architectures can be successfully trained"
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "0Azl7S4KU0b-",
        "outputId": "2ec61e66-51d2-4d01-c945-b7e3faddea13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "#from pandas import datetime\n",
        "import math, time\n",
        "import itertools\n",
        "from sklearn import preprocessing\n",
        "import datetime\n",
        "from operator import itemgetter\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import load_model\n",
        "import keras\n",
        "import h5py\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "print(os.listdir(\"../input\"))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.layers.core'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-87ad934e9127>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.layers.core'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "ggtocQk4U0b-"
      },
      "cell_type": "code",
      "source": [
        "data_df =  pd.read_csv(\"../input/prices-split-adjusted.csv\", index_col = 0)\n",
        "data_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9584f8ed27c94867f5c4a1a36f554c35770717bf",
        "id": "iWgp1XaUU0b_"
      },
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv(\"../input/fundamentals.csv\")\n",
        "df2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3c11e040c3ab738df846f38eb27d64537c497a1b",
        "id": "DoKaAsXZU0b_"
      },
      "cell_type": "markdown",
      "source": [
        "I am extracting the apple stocks only: AAPL"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "30a9e8f016f0fff16e5cc1223a3e916176077120",
        "id": "SV94vTIEU0cA"
      },
      "cell_type": "code",
      "source": [
        "data_df = data_df[data_df.symbol == 'AAPL']\n",
        "data_df.drop(['symbol'],1,inplace=True)\n",
        "data_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c0fd9cca138d6a2f2e685250e40bce62024533e",
        "id": "pKeYtnAlU0cA"
      },
      "cell_type": "code",
      "source": [
        "data_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07c12f402fb3665e301b261047370ec649051fad",
        "id": "jFeRia0rU0cB"
      },
      "cell_type": "code",
      "source": [
        "data_df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f33002b25a435462d40a85f23e633928512317fc",
        "id": "YZnSVGnNU0cB"
      },
      "cell_type": "code",
      "source": [
        "plt.plot(data_df['close'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "04398b0df33164535ffda6dd7ccb1a84fa13bee8",
        "id": "sgFhQx-bU0cB"
      },
      "cell_type": "code",
      "source": [
        "data_df['date'] = data_df.index\n",
        "data_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2733fda3dce158e296264da65e2d8ca34fffbf55",
        "id": "a_EXfo21U0cB"
      },
      "cell_type": "code",
      "source": [
        "data_df['date'] = pd.to_datetime(data_df['date'])pip install DateTime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d470c8e6fa660b3b8dad878a8d4a0c04bfa9f71",
        "id": "KA19fxgEU0cC"
      },
      "cell_type": "code",
      "source": [
        "data_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a2de7ca765456476e28f9ab2b6ccbdd21a2c232e",
        "id": "TYB5TUfzU0cC"
      },
      "cell_type": "markdown",
      "source": [
        "> LSTMs are sensitive to the scale of the input data, specifically when the sigmoid (default) or tanh activation functions are used. It can be a good practice to rescale the data to the range of 0-to-1, also called normalizing."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6c4467ccf683815c8c1c9854e7f9dde7a93c1ff9",
        "id": "fRR5NPIxU0cC"
      },
      "cell_type": "code",
      "source": [
        "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
        "dataset = min_max_scaler.fit_transform(data_df['close'].values.reshape(-1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2cdc2d0f0bf9bd849154f6bae2e2affcf7042df2",
        "id": "DviAnNjDU0cC"
      },
      "cell_type": "code",
      "source": [
        "dataset[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed19f2e090bc572e88689259268b5c2d6fb4dd77",
        "id": "DWghbCDlU0cC"
      },
      "cell_type": "code",
      "source": [
        "# split into train and test sets\n",
        "train_size = int(len(dataset) * 0.7)\n",
        "test_size = len(dataset) - train_size\n",
        "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
        "print(len(train), len(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b518b503213f8779050ff9641d6155c18a50c39a",
        "id": "rZ2dJ7vcU0cC"
      },
      "cell_type": "code",
      "source": [
        "print(len(data_df))\n",
        "print(1233 + 529)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "880eba47d20b5e86e3289301ec70c3c42f8fbc61",
        "id": "e1NH06ovU0cD"
      },
      "cell_type": "markdown",
      "source": [
        "I set the look back date as 15 days, which is the number of previous time steps to use as input variables to predict the next time period"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7b6b6088e81a10df71f2cba199833bf8c536ef49",
        "id": "yJRVYeyGU0cD"
      },
      "cell_type": "code",
      "source": [
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=15):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back, 0])\n",
        "    return np.array(dataX), np.array(dataY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dbbd295863fcd0f9f968e32b2112bc1993be4170",
        "id": "Znc1Ju1pU0cD"
      },
      "cell_type": "code",
      "source": [
        "x_train, y_train = create_dataset(train, look_back=15)\n",
        "x_test, y_test = create_dataset(test, look_back=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "286f91632921a0bd9c22f3c59a63ece054bcf2ee",
        "id": "BhF9DnQ5U0cD"
      },
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "19abce0c37dc0a4895726d35dab6bf43b1e51000",
        "id": "P9Nvp2z2U0cD"
      },
      "cell_type": "markdown",
      "source": [
        "The LSTM network expects the input data (X) to be provided with a specific array structure in the form of: [samples, time steps, features].\n",
        "\n",
        "Currently, this data is in the form: [samples, features] and we are framing the problem as one time step for each sample. We can transform the prepared train and test input data into the expected structure using numpy.reshape() as follows:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ad0f313531b89e9493c927437f27366f0ebfa72",
        "id": "GFTib9XhU0cD"
      },
      "cell_type": "code",
      "source": [
        "x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\n",
        "x_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c1c04ae0f2d7c6d47eceed575723a987856cdbec",
        "id": "9Sn7iY1wU0cE"
      },
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4cdfaa032f4b90bad561ccf68154edac41e9ea38",
        "id": "ymM7gpAUU0cE"
      },
      "cell_type": "markdown",
      "source": [
        "**LSTM**\n",
        "\n",
        "The Long Short-Term Memory network, or LSTM network, is a recurrent neural network that is trained using Backpropagation Through Time and overcomes the vanishing gradient problem.\n",
        "\n",
        "As such, it can be used to create large recurrent networks that in turn can be used to address difficult sequence problems in machine learning and achieve state-of-the-art results.\n",
        "\n",
        "Instead of neurons, LSTM networks have memory blocks that are connected through layers.\n",
        "\n",
        "A block has components that make it smarter than a classical neuron and a memory for recent sequences. A block contains gates that manage the block’s state and output. A block operates upon an input sequence and each gate within a block uses the sigmoid activation units to control whether they are triggered or not, making the change of state and addition of information flowing through the block conditional.\n",
        "\n",
        "There are three types of gates within a unit:\n",
        "\n",
        "* Forget Gate: conditionally decides what information to throw away from the block.\n",
        "* Input Gate: conditionally decides which values from the input to update the memory state.\n",
        "* Output Gate: conditionally decides what to output based on input and the memory of the block.\n",
        "\n",
        "Each unit is like a mini-state machine where the gates of the units have weights that are learned during the training procedure."
      ]
    },
    {
      "metadata": {
        "_uuid": "9ee83c4cdfa51b58aff9fe28f43e552c1ecda9b0",
        "id": "fgUJf0t1U0cE"
      },
      "cell_type": "markdown",
      "source": [
        ">The network has a visible layer with 1 input, a hidden layer with 20 LSTM blocks or neurons, and an output layer that makes a 15 value prediction. The default sigmoid activation function is used for the LSTM blocks. The network is trained for 20 epochs and a batch size of 1 is used."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1962317674b834ebd54be16699a08bb9639c7df6",
        "id": "CPb0tQuOU0cE"
      },
      "cell_type": "code",
      "source": [
        "# create and fit the LSTM network\n",
        "look_back = 15\n",
        "model = Sequential()\n",
        "model.add(LSTM(20, input_shape=(1, look_back)))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(x_train, y_train, epochs=20, batch_size=1, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "444cd8a5403b8f7375578f217ea899746740cb8a",
        "id": "lX6cHOJ9U0cE"
      },
      "cell_type": "code",
      "source": [
        "trainPredict = model.predict(x_train)\n",
        "testPredict = model.predict(x_test)\n",
        "# invert predictions\n",
        "trainPredict = min_max_scaler.inverse_transform(trainPredict)\n",
        "trainY = min_max_scaler.inverse_transform([y_train])\n",
        "testPredict = min_max_scaler.inverse_transform(testPredict)\n",
        "testY = min_max_scaler.inverse_transform([y_test])\n",
        "# calculate root mean squared error\n",
        "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
        "print('Train Score: %.2f RMSE' % (trainScore))\n",
        "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
        "print('Test Score: %.2f RMSE' % (testScore))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9504013c6479db5f1e18ed091eafc1d84454338a",
        "id": "7iRyjJjcU0cF"
      },
      "cell_type": "code",
      "source": [
        "# shift train predictions for plotting\n",
        "trainPredictPlot = np.empty_like(dataset)\n",
        "trainPredictPlot[:, :] = np.nan\n",
        "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = np.empty_like(dataset)\n",
        "testPredictPlot[:, :] = np.nan\n",
        "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
        "# plot baseline and predictions\n",
        "plt.plot(min_max_scaler.inverse_transform(dataset))\n",
        "plt.plot(trainPredictPlot)\n",
        "plt.plot(testPredictPlot)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "722793004b24b338a4827710f7023b78a8bdaa9a",
        "id": "VXabGhG7U0cF"
      },
      "cell_type": "markdown",
      "source": [
        ">reference: https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Time Series Analysis using LSTM Keras",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}